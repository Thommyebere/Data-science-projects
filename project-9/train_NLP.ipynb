{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "118/118 [==============================] - 40s 340ms/step - loss: 0.3917 - accuracy: 0.8483 - val_loss: 1.1580 - val_accuracy: 0.4810\n",
      "Epoch 2/20\n",
      "118/118 [==============================] - 39s 328ms/step - loss: 0.2038 - accuracy: 0.9261 - val_loss: 1.4106 - val_accuracy: 0.4915\n",
      "Epoch 3/20\n",
      "118/118 [==============================] - 37s 313ms/step - loss: 0.1243 - accuracy: 0.9574 - val_loss: 1.9498 - val_accuracy: 0.4674\n",
      "Epoch 4/20\n",
      "118/118 [==============================] - 36s 307ms/step - loss: 0.0796 - accuracy: 0.9739 - val_loss: 2.7682 - val_accuracy: 0.2990\n",
      "Epoch 5/20\n",
      "118/118 [==============================] - 36s 308ms/step - loss: 0.0496 - accuracy: 0.9856 - val_loss: 3.0055 - val_accuracy: 0.3140\n",
      "Epoch 6/20\n",
      "118/118 [==============================] - 38s 319ms/step - loss: 0.0318 - accuracy: 0.9912 - val_loss: 3.2559 - val_accuracy: 0.4104\n",
      "Epoch 7/20\n",
      "118/118 [==============================] - 37s 313ms/step - loss: 0.0199 - accuracy: 0.9945 - val_loss: 3.2023 - val_accuracy: 0.4091\n",
      "Epoch 8/20\n",
      "118/118 [==============================] - 58s 495ms/step - loss: 0.0130 - accuracy: 0.9970 - val_loss: 4.2566 - val_accuracy: 0.3859\n",
      "Epoch 9/20\n",
      "118/118 [==============================] - 63s 530ms/step - loss: 0.0195 - accuracy: 0.9937 - val_loss: 3.7365 - val_accuracy: 0.4214\n",
      "Epoch 10/20\n",
      "118/118 [==============================] - 38s 321ms/step - loss: 0.0122 - accuracy: 0.9967 - val_loss: 3.2959 - val_accuracy: 0.4752\n",
      "Epoch 11/20\n",
      "118/118 [==============================] - 41s 348ms/step - loss: 0.0097 - accuracy: 0.9976 - val_loss: 4.1530 - val_accuracy: 0.3605\n",
      "Epoch 12/20\n",
      "118/118 [==============================] - 39s 332ms/step - loss: 0.0090 - accuracy: 0.9975 - val_loss: 3.1669 - val_accuracy: 0.5000\n",
      "Epoch 13/20\n",
      "118/118 [==============================] - 38s 323ms/step - loss: 0.0180 - accuracy: 0.9949 - val_loss: 4.8119 - val_accuracy: 0.3732\n",
      "Epoch 14/20\n",
      "118/118 [==============================] - 36s 306ms/step - loss: 0.0101 - accuracy: 0.9967 - val_loss: 4.7915 - val_accuracy: 0.3936\n",
      "Epoch 15/20\n",
      "118/118 [==============================] - 37s 311ms/step - loss: 0.0077 - accuracy: 0.9979 - val_loss: 3.9205 - val_accuracy: 0.4591\n",
      "Epoch 16/20\n",
      "118/118 [==============================] - 36s 307ms/step - loss: 0.0164 - accuracy: 0.9949 - val_loss: 2.9553 - val_accuracy: 0.4774\n",
      "Epoch 17/20\n",
      "118/118 [==============================] - 39s 329ms/step - loss: 0.0095 - accuracy: 0.9970 - val_loss: 3.1556 - val_accuracy: 0.4987\n",
      "Epoch 18/20\n",
      "118/118 [==============================] - 38s 320ms/step - loss: 0.0090 - accuracy: 0.9970 - val_loss: 4.0217 - val_accuracy: 0.4538\n",
      "Epoch 19/20\n",
      "118/118 [==============================] - 37s 318ms/step - loss: 0.0091 - accuracy: 0.9971 - val_loss: 4.9740 - val_accuracy: 0.3598\n",
      "Epoch 20/20\n",
      "118/118 [==============================] - 37s 310ms/step - loss: 0.0021 - accuracy: 0.9995 - val_loss: 5.3723 - val_accuracy: 0.4147\n",
      "WARNING:tensorflow:From /Users/eberechukwukathomas/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:1817: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "INFO:tensorflow:Assets written to: 20868189_NLP_model/assets\n"
     ]
    }
   ],
   "source": [
    "#importing necessary libraries\n",
    "from tensorflow.keras.layers import Embedding,LSTM,Dropout,Dense\n",
    "from tensorflow.keras.preprocessing.text import one_hot\n",
    "from tensorflow.keras .preprocessing.sequence import pad_sequences \n",
    "from tensorflow.keras.models import Sequential \n",
    "import numpy as np\n",
    "\n",
    "#Loading the IMDB file \n",
    "\n",
    "if __name__ == \"__main__\": \n",
    "    import urllib.request as req\n",
    "    import tarfile\n",
    "    import os\n",
    "\n",
    "    imdb_url = \"http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n",
    "\n",
    "    save_filename = \"aclImdb_v1.tar.gz\"\n",
    "    if not os.path.exists(save_filename):\n",
    "        req.urlretrieve(imdb_url, save_filename)\n",
    "\n",
    "    imdb_folder = \"aclImdb\"\n",
    "    if not os.path.exists(imdb_folder):\n",
    "        with tarfile.open(save_filename) as tar:\n",
    "            tar.extractall()\n",
    "    import numpy as np\n",
    "    import re\n",
    "#loading the train set into a train file alongside its labels\n",
    "\n",
    "    def get_train_file(data_folder=\"/train\"):\n",
    "        reviews = []\n",
    "        labels = []\n",
    "        for index,sentiment in enumerate([\"/neg/\", \"/pos/\"]):\n",
    "            path = imdb_folder + data_folder + sentiment\n",
    "            for filename in sorted(os.listdir(path)):\n",
    "                with open(path + filename, 'r') as f:\n",
    "                    review = f.read()\n",
    "                    review = review.lower()\n",
    "                    review = review.replace(\"<br />\", \" \")\n",
    "                    review = re.sub(r\"[^a-z ]\", \" \", review)\n",
    "                    review = re.sub(r\" +\", \" \", review)\n",
    "                    reviews.append(review)\n",
    "\n",
    "                    label = [0,0]\n",
    "                    label[index] = 1\n",
    "                    labels.append(label)\n",
    "        return reviews, np.array(labels)\n",
    "    train_reviews, train_labels = get_train_file()\n",
    "    voc_size=10000#vocabulary size\n",
    "    onehot_repr=[one_hot(words, voc_size) for words in train_reviews]#Performing vectorization\n",
    "    sent_len=100#Sentence length\n",
    "    train_reviews=pad_sequences(onehot_repr , maxlen=sent_len)#Padding the vecorized word to fit into the defined sentence length\n",
    "    embed_size=128\n",
    "    X_train=train_reviews[:15000]\n",
    "    X_val=train_reviews[15000:]\n",
    "    y_train=train_labels[:15000]\n",
    "    y_val=train_labels[15000:]\n",
    " \t# 2. Train your network\n",
    "    model=Sequential()\n",
    "    model.add(Embedding(voc_size, embed_size,input_shape=(X_train.shape [1],)))\n",
    "    model.add(LSTM(units=60, activation='tanh',return_sequences=True))\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(LSTM(units=60,activation='tanh',return_sequences=True))\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(LSTM(units=60,activation='tanh',return_sequences=True))\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(LSTM(units=60))\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Dense(units=2, activation='softmax'))\n",
    "    model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\n",
    "    model.fit(X_train, y_train, epochs=20, batch_size=128, validation_data=(X_val, y_val))\n",
    "    model.save('20868189_NLP_model')\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
